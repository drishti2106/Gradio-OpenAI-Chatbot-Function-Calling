{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\nimport openai\n\n# load and set our key\nopenai.api_key = open(\"key.txt\", \"r\").read().strip(\"\\n\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-06T05:02:00.375150Z","iopub.execute_input":"2025-08-06T05:02:00.375370Z","iopub.status.idle":"2025-08-06T05:02:02.555735Z","shell.execute_reply.started":"2025-08-06T05:02:00.375350Z","shell.execute_reply":"2025-08-06T05:02:02.553381Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/3713462189.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# load and set our key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mopenai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"key.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'key.txt'"],"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: 'key.txt'","output_type":"error"}],"execution_count":1},{"cell_type":"code","source":"completion = openai.ChatCompletion.create(\n  model=\"gpt-3.5-turbo\", # this is \"ChatGPT\" $0.002 per 1k tokens\n  messages=[{\"role\": \"user\", \"content\": \"What is the circumference in km of the planet Earth?\"}]\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T05:02:02.556234Z","iopub.status.idle":"2025-08-06T05:02:02.556576Z","shell.execute_reply.started":"2025-08-06T05:02:02.556419Z","shell.execute_reply":"2025-08-06T05:02:02.556431Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(completion)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T05:02:02.558398Z","iopub.status.idle":"2025-08-06T05:02:02.558764Z","shell.execute_reply.started":"2025-08-06T05:02:02.558615Z","shell.execute_reply":"2025-08-06T05:02:02.558634Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"reply_content = completion.choices[0].message.content\nprint(reply_content)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T05:02:02.559570Z","iopub.status.idle":"2025-08-06T05:02:02.559835Z","shell.execute_reply.started":"2025-08-06T05:02:02.559709Z","shell.execute_reply":"2025-08-06T05:02:02.559724Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"message_history = []\n# What is the moon's circumference in km?\nuser_input = input(\"> \")\nprint(\"User's input was: \", user_input)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T05:02:02.560694Z","iopub.status.idle":"2025-08-06T05:02:02.560963Z","shell.execute_reply.started":"2025-08-06T05:02:02.560836Z","shell.execute_reply":"2025-08-06T05:02:02.560856Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"message_history.append({\"role\": \"user\", \"content\": f\"{user_input}\"})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T05:02:02.562357Z","iopub.status.idle":"2025-08-06T05:02:02.562715Z","shell.execute_reply.started":"2025-08-06T05:02:02.562543Z","shell.execute_reply":"2025-08-06T05:02:02.562558Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"completion = openai.ChatCompletion.create(\n  model=\"gpt-3.5-turbo\",\n  messages=message_history\n)\n\n# Now we can print the response:\nreply_content = completion.choices[0].message.content\nprint(reply_content)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T05:02:02.563517Z","iopub.status.idle":"2025-08-06T05:02:02.563782Z","shell.execute_reply.started":"2025-08-06T05:02:02.563651Z","shell.execute_reply":"2025-08-06T05:02:02.563662Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# note the use of the \"assistant\" role here. This is because we're feeding the model's response into context.\nmessage_history.append({\"role\": \"assistant\", \"content\": f\"{reply_content}\"})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T05:02:02.564832Z","iopub.status.idle":"2025-08-06T05:02:02.565169Z","shell.execute_reply.started":"2025-08-06T05:02:02.564997Z","shell.execute_reply":"2025-08-06T05:02:02.565014Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# which moon is that in reference to?\nuser_input = input(\"> \")\nprint(\"User's input was: \", user_input)\nprint()\nmessage_history.append({\"role\": \"user\", \"content\": f\"{user_input}\"})\n\ncompletion = openai.ChatCompletion.create(\n    model=\"gpt-3.5-turbo\",\n    messages=message_history\n)\n\nreply_content = completion.choices[0].message.content\nprint(reply_content)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T05:02:02.566342Z","iopub.status.idle":"2025-08-06T05:02:02.566701Z","shell.execute_reply.started":"2025-08-06T05:02:02.566531Z","shell.execute_reply":"2025-08-06T05:02:02.566548Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"message_history = []\n\ndef chat(inp, role=\"user\"):\n    message_history.append({\"role\": role, \"content\": f\"{inp}\"})\n    completion = openai.ChatCompletion.create(\n        model=\"gpt-3.5-turbo\",\n        messages=message_history\n    )\n    reply_content = completion.choices[0].message.content\n    message_history.append({\"role\": \"assistant\", \"content\": f\"{reply_content}\"})\n    return reply_content\n\nfor i in range(2):\n    user_input = input(\"> \")\n    print(\"User's input was: \", user_input)\n    print(chat(user_input))\n    print()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T05:02:02.567852Z","iopub.status.idle":"2025-08-06T05:02:02.568194Z","shell.execute_reply.started":"2025-08-06T05:02:02.568018Z","shell.execute_reply":"2025-08-06T05:02:02.568033Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import gradio as gr\nimport openai\n\nopenai.api_key = open(\"key.txt\", \"r\").read().strip(\"\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T05:02:02.569231Z","iopub.status.idle":"2025-08-06T05:02:02.569604Z","shell.execute_reply.started":"2025-08-06T05:02:02.569394Z","shell.execute_reply":"2025-08-06T05:02:02.569409Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"message_history = [{\"role\": \"user\", \"content\": f\"You are a joke bot. I will specify the subject matter in my messages, and you will reply with a joke that includes the subjects I mention in my messages. Reply only with jokes to further input. If you understand, say OK.\"},\n                   {\"role\": \"assistant\", \"content\": f\"OK\"}]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T05:02:02.570815Z","iopub.status.idle":"2025-08-06T05:02:02.571158Z","shell.execute_reply.started":"2025-08-06T05:02:02.570974Z","shell.execute_reply":"2025-08-06T05:02:02.570991Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def predict(input):\n    # tokenize the new input sentence\n    message_history.append({\"role\": \"user\", \"content\": f\"{input}\"})\n\n    completion = openai.ChatCompletion.create(\n      model=\"gpt-3.5-turbo\",\n      messages=message_history\n    )\n    #Just the reply text\n    reply_content = completion.choices[0].message.content#.replace('```python', '<pre>').replace('```', '</pre>')\n    \n    message_history.append({\"role\": \"assistant\", \"content\": f\"{reply_content}\"}) \n    \n    # get pairs of msg[\"content\"] from message history, skipping the pre-prompt:              here.\n    response = [(message_history[i][\"content\"], message_history[i+1][\"content\"]) for i in range(2, len(message_history)-1, 2)]  # convert to tuples of list\n    return response","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T05:02:02.572356Z","iopub.status.idle":"2025-08-06T05:02:02.572659Z","shell.execute_reply.started":"2025-08-06T05:02:02.572508Z","shell.execute_reply":"2025-08-06T05:02:02.572526Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# creates a new Blocks app and assigns it to the variable demo.\nwith gr.Blocks() as demo: \n\n    # creates a new Chatbot instance and assigns it to the variable chatbot.\n    chatbot = gr.Chatbot() \n\n    # creates a new Row component, which is a container for other components.\n    with gr.Row(): \n        '''creates a new Textbox component, which is used to collect user input. \n        The show_label parameter is set to False to hide the label, \n        and the placeholder parameter is set'''\n        txt = gr.Textbox(show_label=False, placeholder=\"Enter text and press enter\").style(container=False)\n    '''\n    sets the submit action of the Textbox to the predict function, \n    which takes the input from the Textbox, the chatbot instance, \n    and the state instance as arguments. \n    This function processes the input and generates a response from the chatbot, \n    which is displayed in the output area.'''\n    txt.submit(predict, txt, chatbot) # submit(function, input, output)\n    #txt.submit(lambda :\"\", None, txt)  #Sets submit action to lambda function that returns empty string \n\n    '''\n    sets the submit action of the Textbox to a JavaScript function that returns an empty string. \n    This line is equivalent to the commented out line above, but uses a different implementation. \n    The _js parameter is used to pass a JavaScript function to the submit method.'''\n    txt.submit(None, None, txt, _js=\"() => {''}\") # No function, no input to that function, submit action to textbox is a js function that returns empty string, so it clears immediately.\n         \ndemo.launch()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T05:02:02.574167Z","iopub.status.idle":"2025-08-06T05:02:02.574403Z","shell.execute_reply.started":"2025-08-06T05:02:02.574294Z","shell.execute_reply":"2025-08-06T05:02:02.574303Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import gradio as gr\nimport openai\n\nopenai.api_key = open(\"key.txt\", \"r\").read().strip(\"\\n\")\n\nmessage_history = [{\"role\": \"user\", \"content\": f\"You are a joke bot. I will specify the subject matter in my messages, and you will reply with a joke that includes the subjects I mention in my messages. Reply only with jokes to further input. If you understand, say OK.\"},\n                   {\"role\": \"assistant\", \"content\": f\"OK\"}]\n\ndef predict(input):\n    # tokenize the new input sentence\n    message_history.append({\"role\": \"user\", \"content\": f\"{input}\"})\n\n    completion = openai.ChatCompletion.create(\n      model=\"gpt-3.5-turbo\", #10x cheaper than davinci, and better. $0.002 per 1k tokens\n      messages=message_history\n    )\n    #Just the reply:\n    reply_content = completion.choices[0].message.content#.replace('```python', '<pre>').replace('```', '</pre>')\n\n    print(reply_content)\n    message_history.append({\"role\": \"assistant\", \"content\": f\"{reply_content}\"}) \n    \n    # get pairs of msg[\"content\"] from message history, skipping the pre-prompt:              here.\n    response = [(message_history[i][\"content\"], message_history[i+1][\"content\"]) for i in range(2, len(message_history)-1, 2)]  # convert to tuples of list\n    return response\n\n# creates a new Blocks app and assigns it to the variable demo.\nwith gr.Blocks() as demo: \n\n    # creates a new Chatbot instance and assigns it to the variable chatbot.\n    chatbot = gr.Chatbot() \n\n    # creates a new Row component, which is a container for other components.\n    with gr.Row(): \n        '''creates a new Textbox component, which is used to collect user input. \n        The show_label parameter is set to False to hide the label, \n        and the placeholder parameter is set'''\n        txt = gr.Textbox(show_label=False, placeholder=\"Enter text and press enter\").style(container=False)\n    '''\n    sets the submit action of the Textbox to the predict function, \n    which takes the input from the Textbox, the chatbot instance, \n    and the state instance as arguments. \n    This function processes the input and generates a response from the chatbot, \n    which is displayed in the output area.'''\n    txt.submit(predict, txt, chatbot) # submit(function, input, output)\n    #txt.submit(lambda :\"\", None, txt)  #Sets submit action to lambda function that returns empty string \n\n    '''\n    sets the submit action of the Textbox to a JavaScript function that returns an empty string. \n    This line is equivalent to the commented out line above, but uses a different implementation. \n    The _js parameter is used to pass a JavaScript function to the submit method.'''\n    txt.submit(None, None, txt, _js=\"() => {''}\") # No function, no input to that function, submit action to textbox is a js function that returns empty string, so it clears immediately.\n         \ndemo.launch()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T05:02:02.575455Z","iopub.status.idle":"2025-08-06T05:02:02.575855Z","shell.execute_reply.started":"2025-08-06T05:02:02.575681Z","shell.execute_reply":"2025-08-06T05:02:02.575696Z"}},"outputs":[],"execution_count":null}]}